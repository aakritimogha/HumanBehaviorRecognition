This project showcases a hybrid system that recognizes human behaviors and emotional statesâ€”without using any external datasets. Instead, we rely on rule-based logic, simulated inputs, and facial expression heuristics to infer both physical actions and emotional cues.

ğŸ¯ Objective
To build a lightweight, dataset-free prototype that can detect basic human activities and emotional expressions using simulated data and logical inference.

ğŸ” Recognized Behaviors
Walking
Sitting
Standing
Waving
Jumping

ğŸ˜Š Recognized Emotions
Happy 
Angry 
Surprised 
Sad
Smoking
Emotions are inferred using facial geometry (e.g., eyebrow position, mouth curvature, eye openness) and predefined rules based on expression patterns.

ğŸ› ï¸ Tech Stack
Tool	Purpose
Python	Core logic and scripting
OpenCV	Frame simulation and face analysis
Matplotlib	Graphical output (optional)
VS Code	Development environment

ğŸš€ How It Works
Simulated frames are generated with mock body and facial features.
Behavior rules analyze posture and movement.
Emotion rules evaluate facial landmarks to infer expressions.
Output is visualized with labels and confidence scores.

ğŸ‘¥ Team Members
Aakriti Mogha
Anamika Uniyal 
Aarushi agrawal
Anushka Negi
